id: dbt
namespace: eq-proj

inputs:
  - id: dbt_env
    type: SELECT
    values:
      - dev
      - staging
      - prod
    defaults: dev
    required: true

tasks:
  - id: dbt
    type: io.kestra.plugin.core.flow.WorkingDirectory
    tasks:
      - id: cloneRepository
        type: io.kestra.plugin.git.Clone
        url: https://github.com/plpcout/api-data-pipeline
        branch: dev                             # This branch is gonna be changed latter to be used in CI/CD workflow
        username: "{{kv('GH_USERNAME')}}"     # Uncomment this line for private repos
        password: "{{kv('GH_TOKEN')}}"        # Uncomment this line for private repos

      - id: dbt-build
        type: io.kestra.plugin.dbt.cli.DbtCLI
        containerImage: ghcr.io/kestra-io/dbt-bigquery:latest
        taskRunner:
          type: io.kestra.plugin.scripts.runner.docker.Docker
        projectDir: src/dbt/earthquake_transformations/
        env:
          GCP_PROJECT_ID: "{{envs.gcp_project_id}}"
        profiles: |
          earthquake_transformations:
            outputs:
              {{inputs.dbt_env}}:
                type: bigquery
                dataset: "{{inputs.dbt_env}}_dbt"
                job_execution_timeout_seconds: 300
                job_retries: 1
                location: US
                method: oauth
                priority: interactive
                threads: 1
        commands:
          - dbt deps --project-dir src/dbt/earthquake_transformations/ --target {{inputs.dbt_env}}
          - dbt debug --project-dir src/dbt/earthquake_transformations/ --target {{inputs.dbt_env}}
          - dbt run --project-dir src/dbt/earthquake_transformations/ --target {{inputs.dbt_env}} --vars '{"env":"{{inputs.dbt_env}}"}'